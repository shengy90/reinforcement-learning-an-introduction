{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chapter 1 - Introduction.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMEhHrmsvaVcBKNGkZ1i/sm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shengy90/reinforcement-learning-an-introduction/blob/master/Chapter_1_Introduction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wwpy94hOwTr5",
        "colab_type": "text"
      },
      "source": [
        "# 1.1 Reinforcement Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ueYyyfhwXTl",
        "colab_type": "text"
      },
      "source": [
        "**What is Reinforcement Learning?**\n",
        "- The learner (or agent) is not told which actions to take, but rather through a series of trial and error, the learner discovers which action yields the most reward.\n",
        "- In the most interesting and challenging cases, the learner must learn decisions that affect not only, but also future rewards.\n",
        "- This 'trial-and-error' search and delayed reward are the 2 most distinguising features of Reinforcement Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ig06dRrsxNs7",
        "colab_type": "text"
      },
      "source": [
        "**Reinforcement Learning is different to other branches of machine learning in the following sense:**\n",
        "- unlike *supervised learning*, agents aren't given a set of labelled datasets to extract patterns to make predictions \n",
        "- unlike *unsupervised learning*, it's also not about finding hidden structure within unlaballed datasets \n",
        "- reinforcement learning is about *maximising a reward signal* and finding out the best course of action to perform to achieve that"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UzmH8BtRyNyh",
        "colab_type": "text"
      },
      "source": [
        "# 1.2 Examples of Reinforcement learning problems"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tNX45usByQzL",
        "colab_type": "text"
      },
      "source": [
        "1. **A game of chess** - player needs to plan/ anticipate future moves, and also assess the desirability of any particular moves.\n",
        "2. **New born calf learning to walk** - a new born calf struggles to stand on its feet, but could be running within a few hours of being born.\n",
        "\n",
        "These examples all share the following features:\n",
        "\n",
        "- all involve *interaction* between an active *decision-making agent (the learner)* and its *environment* \n",
        "- the agent seeks to achieve a *goal* despite *uncertainty* about its environment \n",
        "- the agent's action are permitted to affect the future state of the environment \n",
        "- correct choice requires taking into account indirect and delayed consequences of actions, and thus may require foresight/ planning \n",
        "- effect of actions cannot be fully predicted; the agent must constantly monitor and respond to any feedback signals\n",
        "- the agent however can use its experience to improve its performance over time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eHBQASRvz-bN",
        "colab_type": "text"
      },
      "source": [
        "# 1.3 Elements of Reinforcement Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MD0njN360B5D",
        "colab_type": "text"
      },
      "source": [
        "**There are 4 main subelements of an reinforcement learning (RL) system:**\n",
        "1. a `policy`\n",
        "2. a `reward signal`\n",
        "3. a `value function`\n",
        "4. (optionally) a `model` of the environment\n",
        "\n",
        "**`Policy` ðŸ‘®â€â™€ï¸:** defines the learner's way of behaving. A policy is a mapping from perceived states of the environment to the actions to be taken within those states. \n",
        "\n",
        "**`Reward signal` ðŸ¤‘:** defines the goal of a RL problem. On each time step, the environment sends the agent a number (the reward). The agent's sole objective is to *maximise the total reward it receives over the long run*. \n",
        "\n",
        "**`Value Function` âš–ï¸:** whilst a reward signal indicates what is good right now, the `value function` specifies what is good *in the long run*. TL;DR, the `value` of a state represents the *future value discounted to the present state*. \n",
        "\n",
        "> `Rewards` are in a sense *primary* whereas `values`, as predictions of rewards, are secondary. Without reward there can be no value. The only purpose of estimating `values` is to achieve more `reward`. Nevertheless, we're concerned with value when making and evaluating decisions, because we care about getting the greatest rewards over the long run. **Unfortunately, values are hard to detarmine; it must be estimated and re-estimated from the sequences of observations an agent makes over its lifetime**.\n",
        "\n",
        "**`Model` ðŸ—¿:** a model is something that mimics (or predicts) the behaviour of the environment. There are 2 main spectrum for solving RL problems - *model-based* methods and *model-free* methods (the latter are explicitly trial-and-error learners). \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uTw2W0Qt3IoN",
        "colab_type": "text"
      },
      "source": [
        "# 1.5 An Extended Example: Tic-Tac-Toe"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Pog_Tak3La8",
        "colab_type": "text"
      },
      "source": [
        "**We're all familiar with the game of Tic-Tac-Toe. How might we solve this problem?**\n",
        "\n",
        "- **classical *minimax* solution from game theory:** is not applicable because it assumes a particular way of playing by the opponent which is too prescriptive.\n",
        "- **Dynamic programming:** where we iteratively all possible future actions to arrive at an optimum solution. But this requires complete specification of how the opponent will respond which is an option we often do not have access to. \n",
        "- **Learning from experience:** where we play many games against the opponent to learn their behaviour to a certain level of confidence, then try to compute an optimal solution.\n",
        "- **Evolutionary method:** search the space of every possible configuration of Xs and Os and identify for each state, what is the move that has the highest probability of winning (which is estimated by playing against the opponent *a lot of times*).\n",
        "\n",
        "This is a non-exhaustive list, but just an illustration of all possible approaches to the problem. **However (as this is a RL book), we'll focus on an approach that makes use of a `value function`:**\n",
        "1. Set up a table of numbers, 1 for each possible state of the game. \n",
        "2. Each number is the *latest estimate* of the probability of winning from that state - an estimate of the state's `value`.\n",
        "3. Set the initial value of all states as 0.5 (arbitrary, which represents a 50% of winning). Set the state with 3 Xs in a row as 1 (the win state as we have already won), and the state with 3 Os in a row as 0 (as we've lost).\n",
        "4. Play *many* games against the opponent. \n",
        "5. At every turn, we look at the states that would result from each of our possible moves and look up their current values. \n",
        "6. Most of the time, we move *greedily* (selecting the move with highest value). But occasionally, we take a random move (called an *exploratory* move) - it could let us discover new states we've never seen before. \n",
        "7. We keep updating the value of the states after each greedy move to make them more accurate estimates of the probabilities of winning \n",
        "$$V(S_{t}) \\leftarrow V(S_{t} + \\alpha \\Big[V(S_{t+1}) - V(S_{t})\\Big],$$\n",
        "where $\\alpha$ (a small positive fraction) is the learning rate."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZIl5oG_wB3kO",
        "colab_type": "text"
      },
      "source": [
        "**The key difference between evolutionary methods vs value-function methods:**\n",
        "- evolutionary methods relies on playing the game many many times to obtained the unbiased probability estimates (of winning) of every state. Only the 'trained model' is used in production. It doesn't take into account of/ respond to what happened during the game. \n",
        "- value function methods however allows individual states to be evaluated, taking into account of new situations/ information it's never seen before. "
      ]
    }
  ]
}