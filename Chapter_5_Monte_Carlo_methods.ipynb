{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chapter 5 - Monte Carlo methods",
      "provenance": [],
      "authorship_tag": "ABX9TyMWSUs3leQE7kL7WAQE07Lo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shengy90/reinforcement-learning-an-introduction/blob/master/Chapter_5_Monte_Carlo_methods.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jwjsD6SZyVbT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SrHQaf19yerp",
        "colab_type": "text"
      },
      "source": [
        "# 5. Monte Carlo Methods\n",
        "- Monte Carlo methods are a class of methods that do not assume complete knowledge of the environment\n",
        "- it requires only experience, i.e. a sample sequences of states, actions and rewards from actual or simulated interaction with an environment \n",
        "- it requires no prior knowledge of the environment's dynamics\n",
        "- Monte Carlo methods sample and average returns for each state-action pair \n",
        "- unlike DP where we *computed* value functions, Monte Carlo methods *learn* the value functions from sample returns \n",
        "- however like DP, it's still an iterative process of 'policy evaluation' and 'policy improvements'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X5ZKomBZzfgq",
        "colab_type": "text"
      },
      "source": [
        "# 5.1 Monte Carlo Prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sb_1ra_bzfS3",
        "colab_type": "text"
      },
      "source": [
        "**Basic principle of Monte Carlo prediction**\n",
        "- recall that value of a state is just the expected return starting from that state \n",
        "- so with large enough iteration, simply averaging the returns observed after visits to the state should converge to the expected value!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_zSbQ_Um1oWe",
        "colab_type": "text"
      },
      "source": [
        "**Example:**\n",
        "- suppose we wish to estimate $v_\\pi(s)$ - the value of state $s$ under policy $\\pi$, given a set of episodes obtained by following $\\pi$ and passing through $s$ \n",
        "- each occurence of state $s$ is called a *visit* to state $s$\n",
        "- $s$ may be visited multiple times in the same episode \n",
        "- *first-visit MC method* estimtes $v_\\pi(s)$ = average returns following first visits to s\n",
        "- *every-visist MC method* estimates average returns following **all** visits to s\n",
        "\n",
        "We'll focus 'first-visit MC method' in this chapter."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_-zX4Q0S2afo",
        "colab_type": "text"
      },
      "source": [
        "##### **First Visit MC Prediction Psedo code**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SqN8qNFL2tz_",
        "colab_type": "text"
      },
      "source": [
        "`Input`: a policy $\\pi$ to be evaluated \n",
        "\n",
        "`Initialise`: \n",
        "- $V(s) \\epsilon \\mathbb{R}$ arbitrarily for all $s \\epsilon S$\n",
        "- Returns(s) : an empty list for all $s \\epsilon S$\n",
        "\n",
        "> Loop forever (for each episode):\n",
        "- Generate an episode following $\\pi$ : $S_0$, $A_0$, $R_1$, $S_1$,....,$S_{T-1}$,$A_{T-1}$,$R_T$\n",
        "- Let G = 0\n",
        "    - Loop for each step of episode, $t = T-1, T-2 ... 0$:\n",
        "        - $G = \\gamma G + R_{t+1}$\n",
        "        - Unless $S_t$ appears in $S_0$, $S_1$....,$S_{t-1}$:\n",
        "            - Append G to Returns($S_t$)\n",
        "            - $V(S_{t})$ = average(Returns($S_t$))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1-nYwa4l4HA5",
        "colab_type": "text"
      },
      "source": [
        "**Both first-visit MC and every-visit MC converges to $v_\\pi(s)$ as the number of visits goes to infinity**. Each return is assumed to be an independent and identically distributed estimate of $v_\\pi(s)$. By law of large number, the average of this sequence will therefore converge to the expevted value, with a standard deviation of $\\frac{1}{\\sqrt{n}}$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I_Reh-mzE2iW",
        "colab_type": "text"
      },
      "source": [
        "# 5.2 Monte Carlo Estimation of Action Values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BEHars3fE8Dg",
        "colab_type": "text"
      },
      "source": [
        "**In situations where model is not available,** it's useful to estimate action values (values of state-action pairs) rather than state values. **With a model however**, state values alone are enough to determine a policy. \n",
        "\n",
        "As mentioned, Monte Carlo methods are super useful when a model of the environment is not available. For this reason, we're more interested in estimating $q_*$. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RefSiwmKFokk",
        "colab_type": "text"
      },
      "source": [
        "**Recall in policy evaluation problem**, we estimate $q_{\\pi}(s,a)$ - the expected return when starting in state s and taking action a folowing policy $\\pi$. This is similar in Monte Carlo methods, except that we now talk about 'visits to a state-action pair' rather than to a state. \n",
        "\n",
        "A `state-action pair (s,a`) being visited means state s is visited **and** action a is taken in it. In *first-visit MC*, we average returns following the first time `(s,a)` was visited. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y9dipr0KGfF5",
        "colab_type": "text"
      },
      "source": [
        "**What if many `state-action` pairs were never visited?** \n",
        "\n",
        "With no returns to average, the MC estimates of other actions will not improve with experience! Recall the trade off between 'optimisation' and 'exploration' in chapter 2? For policy evaluation to work, we must ensure *continual exploration*. \n",
        "\n",
        "For now, let's assume that every state-action pairs have a non-zero probability of being selected, such that as time $\\to \\infty$, all state-action pairs will be selected $\\infty$ times (the assumption called *`exploring starts`*). In future chapters, we can expand this to only consider stochastic policies with nonzero posibility of selecting all actions in each state. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M52Zwx2cHteA",
        "colab_type": "text"
      },
      "source": [
        "# 5.3 Monte Carlo Control"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "csq1JUoGHu-4",
        "colab_type": "text"
      },
      "source": [
        "# 5.4 Monte Carlo Control without Exploring Starts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qOaN0qpZH51Y",
        "colab_type": "text"
      },
      "source": [
        "#5.5 Off-policy prediction via Importance Sampling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nurjm20kH-hc",
        "colab_type": "text"
      },
      "source": [
        "# 5.6 Incremental Implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cqt85WmHIA-t",
        "colab_type": "text"
      },
      "source": [
        "# 5.7 Off-policy Monte Carlo Control"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9WjwMBDUIEsq",
        "colab_type": "text"
      },
      "source": [
        "# 5.8 Discounting-aware Importance Sampling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "43Jhwx-rIH2t",
        "colab_type": "text"
      },
      "source": [
        "# 5.9 Per-decision Importance Sampling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MEw1WUktydNH",
        "colab_type": "text"
      },
      "source": [
        "# BAHASDASDA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9kAPw6w-yeD6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}