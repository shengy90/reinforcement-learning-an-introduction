{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chapter 6: Temporal-Difference Learning .ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOLSQaAtjdAD3Hb3vPxPQ+g",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shengy90/reinforcement-learning-an-introduction/blob/master/Chapter_6_Temporal_Difference_Learning_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NC4EukplIzOL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U7fMoELYKdB0",
        "colab_type": "text"
      },
      "source": [
        "# Temporal-Difference Learning:\n",
        "- A combination of Monte Carlo and Dynamic Programming ideas \n",
        "- TD methods can learn directly from raw experience without a model of the environment's dynamics \n",
        "- TD methods update estimpates based on other learned estimates \n",
        "\n",
        "**2 types of problems in TD methods:**\n",
        "1. `policy evaluation` or `prediction`: estimating the value function $v_{\\pi}$ for a given policy $\\pi$.\n",
        "2. `finding optimal policy` or `control` : using generalised policy iteration to discover the optimal policy \n",
        "\n",
        "**2 ways of finding the optimal policy:**\n",
        "- `on-policy` : refers to the value-action function $q(s,a)$ from actions taken under the current policy.\n",
        "- `off-policy` refers to the value-action function $q(s,a)$ that includes all types of action, including explarotary ones."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mo2ura5sLHVo",
        "colab_type": "text"
      },
      "source": [
        "# 6.1 TD Prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LSW37f7aLLQn",
        "colab_type": "text"
      },
      "source": [
        "Given some experience following policy $\\pi$, we update their estimate of $v_\\pi$ for the nonterminal states $S_t$:\n",
        "\n",
        "$$\n",
        "V(S_t) \\leftarrow V(S_t) + \\alpha \\big[R_{t+1} + \\gamma V(S_{t+1}) - V(S_{t}) \\big]\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4C3mlgnILz0M",
        "colab_type": "text"
      },
      "source": [
        "On transition to the state $S_{t+1}$ (and thereby receiving $R_{t+1}$), we update $S_{t}$ immediately."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-GUfFC2Y4S_6",
        "colab_type": "text"
      },
      "source": [
        "We can think of TD update as *sort of an error*, because it's kind of measuring the difference between the estimated value of $S_t$ and a better estimate $R_{t+1} + \\gamma V(S_{t+1})$. \n",
        "\n",
        "The update term $R_{t+1} + \\gamma V(S_{t+1}) - V(S_t)$ is also known as the `TD error`, $\\delta_t$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l-75Zi3c53gS",
        "colab_type": "text"
      },
      "source": [
        "**Note that:**\n",
        "- the TD error term at each timestamp is the error in the estimate *made at that time*\n",
        "- The TD error term depends on the next state and the next reward and is not actualy available until one step later! "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hEEvP4_VMhkw",
        "colab_type": "text"
      },
      "source": [
        "# 6.2 Advantages of TD Prediction Methods"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1oWh-0bj78Xn",
        "colab_type": "text"
      },
      "source": [
        "- TD methods update theri estimates based in part on other estimates. They learn a guess *from a guess*.\n",
        "\n",
        "**compared to DP methods:**\n",
        "- TD methods don't require a model of the environment, its rewards, and probability distributions of the next state. \n",
        "- TD methods are implemented in an online incremental fashion - they update in real time instead of waiting for the episode to finish (Like in Monte Carlo methods)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v3ZCJuK8Mmb4",
        "colab_type": "text"
      },
      "source": [
        "# 6.3 Optimality of TD(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MvT6yq4c8oz7",
        "colab_type": "text"
      },
      "source": [
        "**Suppose that there is available only a finite amount of experience, say 10 episodes, or 100 time steps...**\n",
        "- common approach with incremental learning is to present the experience repeatedly until method converges \n",
        "- Given an approximate value function V, the *update terms* are computed for every timestep for a nonterminal state but the value function is only changed once, by the sum of all the increments\n",
        "- then all available experience is processed again with the new value function to produce a new update term \n",
        "\n",
        "> This is also called 'batch updating' because updates are only made after processing each complete batch of training data\n",
        "\n",
        "**Under batch updating**\n",
        "- TD(0) converges deterministically to a single answer independent of the learning rate, $\\alpha$, as long as $\\alpha$ is sufficiently small"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vxv79obUMpzv",
        "colab_type": "text"
      },
      "source": [
        "# 6.4 Sarsa: On-policy TD Control"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j0clZvkgpheD",
        "colab_type": "text"
      },
      "source": [
        "**Recall:**\n",
        "1. An episode consists of an alternating sequence of states and state-action pairs:\n",
        "    $$... S_t \\to A_t \\to R_{t+1} \\to S_{t+1} \\to A_{t+1} \\to ...$$\n",
        "2. We're interested in learning the action-value pair $q(s,a)$ instead of the state-value pair $v(s,a)$, because the latter requires us to know everything about the environment, which is a criteria we're relaxing for the TD method.\n",
        "\n",
        "\n",
        "$$\n",
        "Q(S_t, A_t) \\leftarrow \n",
        "Q(S_t, A_t) + \\alpha \\big[R_{t+1} + \n",
        "\\gamma Q(S_{t+1}, A_{t+1}) - Q(S_{t},A_{t}) \\big]\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c1F7QEeixJrZ",
        "colab_type": "text"
      },
      "source": [
        "**SARSA**:\n",
        "\n",
        "- This update is done after every transition from a nonterminal state $S_t$. If $S_{t+1}$ is terminal, then $Q(S_{t+a},A_{t+1})$ is 0.\n",
        "- This rule uses every element of the quintuple of events : $(S_t, A_t, R_{t+1}, S_{t+1}, A_{t+1})$ and thus, *SARSA*!\n",
        "- As this is an on-policy method, we contiually estimate $q_\\pi$ for the behaviour policy, $\\pi$ and change $\\pi$ towards the greediness with respect to $q_\\pi$\n",
        "- SARSA converges to the optimal policy and action-value function as long as all state-action pairs are visited infinite amount of times if $t \\to \\infty$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JjCjzxNAzQoV",
        "colab_type": "text"
      },
      "source": [
        "**Pseudo-code**\n",
        "\n",
        "- Parameters: $\\alpha \\in (0,1], \\text{small } \\epsilon > 0$ \n",
        "- Initialise $Q(s,a)$ for all $s \\in S^+$, $\\alpha \\in A(s)$, arbitrarily except $Q(\\text{terminal},\\cdot )=0$ \n",
        "- Loop for each episode: \n",
        "    - Initialise $S$\n",
        "    - Choose $A$ from $S$ using policy derived from Q (e.g. $\\epsilon$-greedy)\n",
        "    - Loop for each step of episode:\n",
        "        - Take action A, observe R, S'\n",
        "        - Choose A' from S' using policy derived from Q (e.g. $\\epsilon$-greedy)\n",
        "        - Update Q(S,A) $\\leftarrow$ Q(S,A) + $\\alpha$[R + $\\gamma$Q(S',A') - Q(S,A)]\n",
        "        - Update $S \\leftarrow S', A \\leftarrow A'$\n",
        "    - until S is terminal "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iBg_yHKPMuix",
        "colab_type": "text"
      },
      "source": [
        "# 6.5 Q-learning: Off-policy TD Control"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ep_vCJct1VJ1",
        "colab_type": "text"
      },
      "source": [
        "$$\n",
        "Q(S_t,A_t) \\leftarrow Q(S_t, A_t) + \n",
        "\\alpha \\big[ R_{t+1} + \\gamma \\max_a Q(S_{t+1}, a) - Q(S_t, A_t) \\big]\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xGBDhlcR1s6l",
        "colab_type": "text"
      },
      "source": [
        "In this case, the learned action-value function directly approximates $q*$, the optimal action-value function, independent of the policy being followed, meaning all it needs for convergence is that all pairs continue to be updated. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h04fODMe0s5w",
        "colab_type": "text"
      },
      "source": [
        "**Pseudo-code**\n",
        "- Parameters: step size $\\alpha \\in (0,1], \\text{ small } \\epsilon \\gt 0$\n",
        "- Initialise $Q(s,a)$ for all $s \\in S^+, a \\in A(s)$, arbitrarily except $Q(terminal, \\cdot) = 0$ \n",
        "- Loop for each episode:\n",
        "    - Initialise $S$\n",
        "    - Loop for each step of episode:\n",
        "        - Choose A from S using policy derived from Q e.g. ($\\epsilon$-greedy)\n",
        "        - Take action A, observe R, S'\n",
        "        - Update Q(S,A) $\\leftarrow$ Q(S,A) + $\\alpha$[R + $\\gamma \\max_{a}$Q(S',a) - Q(S,A)]\n",
        "        - Update $S \\leftarrow S', A \\leftarrow A'$\n",
        "    - until S is terminal "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HHpYlr-TMx3A",
        "colab_type": "text"
      },
      "source": [
        "# 6.6 Expected Sarsa"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eqA7aBVf2YH_",
        "colab_type": "text"
      },
      "source": [
        "A modification of Q-learning, in that it takes the *expected* value instead of the *max* to update Q(S,A):\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "Q(S_t,A_t)\n",
        "& \\leftarrow Q(S_t,A_t) + \\alpha \\big[ \n",
        "    R_{t+1} + \n",
        "    \\gamma \\mathbb{E}_\\pi [Q(S_{t+1},A_{t+1} | S_{t+1} - Q(S_t,A_t)] \n",
        "    \\big] \\\\\n",
        "& \\leftarrow Q(S_t,A_t) + \\alpha \\big[ \n",
        "    R_{t+1}\n",
        "    + \\gamma \\sum_{a} \\pi(a|S_{t+1})Q(S_{t+1},a) - Q(S_t,A_t) \n",
        "    \\big] \\\\\n",
        "\\end{align}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GMMCjd2b3t63",
        "colab_type": "text"
      },
      "source": [
        "This algorithm moves *deterministically* in the same direction as Sarsa moves *in expectation*, therefore is called *expected SARSA*. It's more computationally complex, but eliminates the variance due to the random selection of $A_{t+1}$ and therefore, performs slightly better than SARSA."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KwEesBhDM0PN",
        "colab_type": "text"
      },
      "source": [
        "# 6.7 Maximisation Bias and Double Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xjqrjjbv6QP7",
        "colab_type": "text"
      },
      "source": [
        "All of these methods attempt to maximise their target policies. For Q-learning, it's the greedy policy given current action values (defined with a max). In SARSA, it's the $\\epsilon$-greedy policy. This however can lead to positive bias. \n",
        "\n",
        "Imagine if the true value of q(s,a) are all zero but the estimates Q(s,a) are uncertain and distributed around 0. The maximum of the true values is zero, but the maximum of the estimate is positive (therefore a positive bias!). This is called *maximisation bias*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S6ZdnZgY6h-x",
        "colab_type": "text"
      },
      "source": [
        "One way to elimiate this maximisation bias is through something called **Double Learning**. Basically, we learn 2 estimates, but at each timestep only 1 of the 2 are randomly updated. Then, we could just use the average of both estimates as our expectation of the maximum. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "meCjGRZ57ULQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}