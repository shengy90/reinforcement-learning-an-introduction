{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chapter 2 - Multiarmed Bandits.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNyDxWUAOCIS+M50+Ehnj7e",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shengy90/reinforcement-learning-an-introduction/blob/master/Chapter_2_Multiarmed_Bandits.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lFD5FRZo8ajt",
        "colab_type": "text"
      },
      "source": [
        "In this chapter, we look at the simplest form of a Reinforcement Learning problem - the K-armed bandit problem. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Kw5IvnZ8jwY",
        "colab_type": "text"
      },
      "source": [
        "#2.1 A k-armed Bandit Problem"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UuxUrDf-8qV5",
        "colab_type": "text"
      },
      "source": [
        "**Problem specification**\n",
        "- you have to make a choice amongst k different possible action\n",
        "- after each choice, you receive a numerical reward chosen from a *stationary probability distribution* that depends on your chosen action\n",
        "- your objective is to maximise the *expected total reward* over some time period "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n5evxTql9KtW",
        "colab_type": "text"
      },
      "source": [
        "**Important concepts:**\n",
        "\n",
        "- each of the *k* actions has an expected (or mean) reward given that the action was selected: $$q_{*}(a) = \\mathbb{E}\\big[R_{t} | A_{t} \\dot= a\\big]$$ where \n",
        "$A_{t}$ is the action selected at time $t$ and its corresponding reward being $R_{t}$, and $q_{*}(a)$ represents the expected reward given that $a$ was selected *(aka value of action a)*.\n",
        "\n",
        "- we may *(or may not, though we might have estimates)* know with certainty the value of each action. Let's call the *estimated value of action $a$ and time step $t$ as $Q_{t}(a)$*, where $Q_{t}(a)$ is as close as $q_{*}(a)$ as possible. \n",
        "\n",
        "- *greedy actions* are actions where the *estimated value* $Q_{t}(a)$ is the greatest. Choosing a *greedy* action is also known as *exploiting*. *Exploration* refers to choosing a *nongreedy* action. Reward might be shorter in the short run when exploring, but it *could* be higher in the long run as it allows us to discover new states that might have higher rewards. \n",
        "\n",
        "- there are many ways to 'balance' (or optimise) between exploration and exploitation, though many of these methods assume *stationarity*, which may or may not apply\n",
        "\n",
        "In this chapter, we'll explore simple balancing methods for the k-armed bandit problem and show that they work better than *always-exploit* methods."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rH7t9DHkAd4k",
        "colab_type": "text"
      },
      "source": [
        "# 2.2 Action-value Methods"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "txc0YKctBCBv",
        "colab_type": "text"
      },
      "source": [
        "**`Action-value methods`** are methods that *estimate the values of action* and uses these estimates to *make decisions*. An example:\n",
        "\n",
        "\n",
        "\n",
        "$$Q_{t}(a) \\dot = \\frac{\\text{sum of rewards when a taken prior to t}}{\\text{number of times a taken prior to t}} = \\frac{\\sum_{i=1}^{t-1} R_{i} \\cdotp \\mathbb{1}_{A_{i=a}}}{\\sum_{i=1}^{t-1} \\mathbb{1}_{A_{i=a}} }$$\n",
        "\n",
        "where:\n",
        "- $\\mathbb{1}_{predicate}$ = random variable that is 1 if predicate is true and 0 if not \n",
        "- if denominator is 0, we define $Q_{t}(a)$ as some default e.g. 0 (to avoid div by zero error) \n",
        "- As denominator tends to infinity (choosing action a many many times), then by definition $Q_{t}(a)$ tends to $q_{*}(a)$\n",
        "- this is also known as the `sample-average` method as it essentially is estimating action values by average the rewards of a by sampling action a\n",
        "\n",
        "**Recall that `greedy` action refers to the action with the highest value**, we can define a `greedy` action as:  \n",
        "\n",
        "$$A_{t} \\dot = \\arg\\max_a Q_{t}(a)$$\n",
        "\n",
        "An alternative to always greedy, is to behave greedily *most of the time*, but every once in a while with a small probability $\\varepsilon$, select a random non-greedy action instead (regardless of its action value). We call this method the $\\varepsilon$-greedy method.\n",
        "\n",
        "\n",
        "**Benefits of $\\varepsilon$-greedy methods**\n",
        "- as $t$ tends to $\\infty$, by definition, all actions would evebntually be selected $\\infty$ times, ensuring that $Q_t(a) \\to q_{*}(a)$ for all $(a)$\n",
        "- this also implies that probability of selecting the optimal action converges to greater than $1-\\varepsilon$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cyklckgpAh7P",
        "colab_type": "text"
      },
      "source": [
        "#2.3 The 10-armed Testbed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C0mpZE2xLLpf",
        "colab_type": "text"
      },
      "source": [
        "TODO: Recreate the example to show that $\\varepsilon > 0$ has higher average reward compared to $\\varepsilon = 0$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sl_gWL0bLtBE",
        "colab_type": "text"
      },
      "source": [
        "- The advantage of $\\varepsilon$-greedy methods depends on the reward variance; noisier rewards takes more exploration to find the optimal action, meaning $\\varepsilon$-greedy methods will fare far better than greedy methods. \n",
        "- On the other hand, if reward variance was 0, then greedy method might perform best albeit only in stationary setting (recall our stationarity assumption). In a nonstationarity setting, any learned experience would be outdated and without exploration, we'll never find the new optimal policy. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZhzdsrmbMk6F",
        "colab_type": "text"
      },
      "source": [
        "TODO: exercises 2.2 and 2.3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-IhiidPkAlnC",
        "colab_type": "text"
      },
      "source": [
        "#2.4 Incremental Implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t2TOOLYsMpP0",
        "colab_type": "text"
      },
      "source": [
        "#### **Computing the sample averages of observed rewards in a computationally effficient manner**\n",
        "\n",
        "For any given action:\n",
        "\n",
        "$$Q_{n} \\dot = \\frac{1}{n-1} \\sum_{i=1}^{N=n-1} R_{i}$$\n",
        "\n",
        "where $R_{i}$ refers to the reward after the $i$th selection of this action, and $Q_n$ is the estimate of its action after being selected $n$-1 times. \n",
        "\n",
        "The simplest solution would simply to keep every $R$s observed. But this naive implementation would grow over time as t tends to a very large number. Instead we could use 'dynamic programming' to compute $Q_{n+1}$ given $Q_{n}$ and $R_{n}$ : $Q_{n+1} = Q_n + \\frac{1}{n}\\big[R_n - Qn]$.\n",
        "\n",
        "> **TL;DR** New Estimate $\\leftarrow$ Old Estimate + Step Size [Target - Old Estimate], where [Target - Old Estimate] is also called the 'error' in the estimate, and is *reduced* by taking a step towards the *target*. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_dbTXQOgOTPd",
        "colab_type": "text"
      },
      "source": [
        "##### **Derivation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AI0QMTFwOY_6",
        "colab_type": "text"
      },
      "source": [
        "![derivation](https://github.com/shengy90/reinforcement-learning-an-introduction/blob/master/misc/fig1.png?raw=true)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tzJ9RhjpRJqp",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C1WuNgEPApPQ",
        "colab_type": "text"
      },
      "source": [
        "#2.5 Tracking a Nonstationary Problem"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NPCv1_IvRokC",
        "colab_type": "text"
      },
      "source": [
        "The above examples are stationary examples where reward probabilities *do not change over time*. This is however not the case in real world application most of the time. When reward probabilities aren't constant over time, it makes more sense to give more weight to recent than long-past rewards. \n",
        "\n",
        "A common way to do this is to use a constant 'step-size' parameter, $\\alpha$:\n",
        "\n",
        "$$ Q_{n+1} \\dot = Q_{n} + \\alpha \\big[R_n - Q_n \\big]$$\n",
        "\n",
        "where $\\alpha \\epsilon (0,1]$. $Q_{n+1}$ now becomes a weighted average of past rewards and initial estimate $Q_1$. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OS4X2pthSpyL",
        "colab_type": "text"
      },
      "source": [
        "##### **Derivation (Todo)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cTL8kouMS2VK",
        "colab_type": "text"
      },
      "source": [
        "#### **Varying $\\alpha$**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hh-w1uzgS6Xh",
        "colab_type": "text"
      },
      "source": [
        "There might be times where we wish to vary $\\alpha$. In this case, there are be 2 conditions that we might care about:\n",
        "\n",
        "1. $\\sum_{n=1}^{\\infty} \\alpha_{n}(a) = \\infty$ : this guarantees that alpha would be large enough to overcome any initial conditions/ random fluctions, i.e. not get stuck in local minima and not reach the global minima.\n",
        "\n",
        "2. $\\sum_{n=1}^{\\infty} \\alpha_{n}(a)^2 \\lt \\infty$ : this guarantee that estimates converge. **For non-stationary situations, we might not want the estimates to converge** but rather, to vary based on the most recent rewards! "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HT5QKHMgUtRx",
        "colab_type": "text"
      },
      "source": [
        "#### **TODO:PROGRAMMING EXAMPLES**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "15h2xLtJUxRk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#🤷‍♂️"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sSdXmoK_AsXq",
        "colab_type": "text"
      },
      "source": [
        "#2.6 Optimistic Initial Values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a0FuKTlJVAYK",
        "colab_type": "text"
      },
      "source": [
        "- the above methods to some extend, depends on the initial action-value estimates $Q_1(a)$, although in sample-average with constant $\\alpha$, this bias decrease overtime. \n",
        "- this bias could be helpful in some cases, e.g. supplying some prior knowledge of what level of rewards to be expected etc\n",
        "- it could also used as a simple way to encourage exploration in stationary problems by setting it to a 'high' value. We call this an `optimistic initial value`. \n",
        "- It is however not suited in nonstationary problems as (recall above) the influence of the bias decreases over time (whereas we would want the agent to routinely explore in nonstationary setting). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bXsQhi7hY6H1",
        "colab_type": "text"
      },
      "source": [
        "**TODO: EXAMPLES**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Arzy3GMmAvmL",
        "colab_type": "text"
      },
      "source": [
        "# 2.7 Upper-Confidence-Bound Action Selection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xCPZRDP0Y9Fe",
        "colab_type": "text"
      },
      "source": [
        "- $\\epsilon$-greedy methods are better than greedy ones becayse they force the agent to explore new actions which could uncover higher value states never seen before.\n",
        "- but $\\epsilon$-greedy methods explores *indiscriminately* with no preference for those that are *nearly greedy* vs *particularly uncertain*\n",
        "- it might be better to select non-greedy actions based on how likely they are to actually be optimal.\n",
        "\n",
        "One way to effectively do this:\n",
        "\n",
        "$$A_t \\dot = \\arg\\max_a \\Bigg[Q_t(a) + c \\sqrt{\\frac{\\ln t}{N_{t}(a)}} \\Bigg],$$\n",
        "\n",
        "where $N_t(a)$ denotes the number of times action a was selected prior to time t, whilst c controls the degree of exploration. The square-root term is a measure of the uncertainty (variance) in the estimate of $a$'s value. Each time a is selected ($N_t(a)$ increases), this uncertainty is being reduced, whilst an action other than a being selected, $\\ln t$ increases but not $N_t(a)$. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HOz5K8QMAzas",
        "colab_type": "text"
      },
      "source": [
        "#2.8 Gradient Bandit Algorithms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bhPgowP3eYxx",
        "colab_type": "text"
      },
      "source": [
        "So far, we've conosidered methods that estimate action values and use these estimates to select actions. There are other ways to do this, e.g. adding a 'preference' of one action over another. A way to do this is to apply a preference based on a *soft-max distribution*.:\n",
        "\n",
        "$$ Pr(A_t=a) \\dot = \\frac{e^{H_t(a)}}{\\sum_{b=1}^k e^{H_t(b)}} \\dot = \\pi_{t}(a)$$\n",
        "\n",
        "where $\\pi_{t}(a)$ refers to the probability of taking action $a$ at time $t$ and $H_{t}(a)$ refers to the preference such that:\n",
        "\n",
        "$$H_{t+1}(a) \\dot = H_t(a) - \\alpha(R_t - \\bar R_t) \\pi_t(a) \\space \\space \\space \\space \\text{for all } a \\neq A_t,$$\n",
        "\n",
        "\n",
        "where $\\bar R_t \\epsilon \\mathbb{R}$ is the average of the rewards up $t-1$ (with $\\bar R_1 = R_1$). $\\bar R_t$ serves as a 'baseline' for which the reward is compared; if the reward is higher than the baseline, then probability of taking $A_t$ in the future is increased and vice versa. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q6ChhLzqA28P",
        "colab_type": "text"
      },
      "source": [
        "#2.9 Associative Search (Contextual Bandits)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7UTi_DLhpxW",
        "colab_type": "text"
      },
      "source": [
        "So far, we've only looked at `nonassociate tasks`, i.e. tasks in which we don't need to *associate different actions with different situations*. In some cases where we might want to map situations to actions that are best in those situations. For example in a self-driving car scenario, you could have *if traffic light is red, stop; else go* etc. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pbNIcw-o8jDW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}