{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chapter 3 - Finite Markov Decision Processes.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyN8LJG8cLfqD9yKTEtNjMAj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shengy90/reinforcement-learning-an-introduction/blob/master/Chapter_3_Finite_Markov_Decision_Processes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "73-3zYOW1Sfh",
        "colab_type": "text"
      },
      "source": [
        "**`Markov Decision Processess`** or MDPs:\n",
        "- involve both evaluative feedback (like the bandit problems) and associative aspect (choosing different actions in different situations)\n",
        "- classical formalisation of *sequential decision making*, taking into consideration both *immediate* and *delayed* rewards \n",
        "- we estimate the value $q_*(state, action)$ instead of just $q_*(action)$\n",
        "\n",
        "**Things we'll be looking into:**\n",
        "- returns \n",
        "- value functions \n",
        "- Bellman equations "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "br_KXxG7i3IC",
        "colab_type": "text"
      },
      "source": [
        "# 3.1 The Agent-Environment Interface"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ytAD4lAd2IJy",
        "colab_type": "text"
      },
      "source": [
        "**Some nomenclature:**\n",
        "- **`agent`** : the learner/ decision maker \n",
        "- **`environment`**: thing an `agent` interacts with. Basically everything in the system outside of the agent. \n",
        "\n",
        "![fig1](https://github.com/shengy90/reinforcement-learning-an-introduction/blob/master/misc/ch3fig1.png?raw=true)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8VPyZgjN2xxG",
        "colab_type": "text"
      },
      "source": [
        "**Agent and environment interact at each discrete timesteps:** $t = 0,1,2...$. At each timestep, the `agent` receives some representation of the environment's `state` $S_t$ and on that basis, selects an `action` $A_{t}$. Then, as a consequence of the action, the `agent` receives a `reward` $R_{t+1}$ and finds itself in a new state, $S_{t+1}$.\n",
        "\n",
        "\n",
        "**The agent and the environment therefore give rise to a sequence** aka `trajectory`:\n",
        "\n",
        "$$S_0, A_0, R_1, S_1, A_1, R_2, S_2, A_2....$$\n",
        "\n",
        "\n",
        "**A finite MDP** (sets of states $\\{S, A, R\\}$ **all** have a finite number of elements. In this case, random variables $R_t$ and $S_t$ have well defined discrete probability distributions that depends **only on the preceding state and action**:\n",
        "\n",
        "$$p(s',r | s,a) \\dot = P \\{S_t=s', R_t=r | S_{t-1}=s, A_{t-1}=a\\},$$\n",
        "\n",
        "for all $s', s \\epsilon S$, $r \\epsilon R$ and $a \\epsilon A(s)$. \n",
        "\n",
        "The function $p$ defines the `dynamics` of the MDP. The dynamaics function $p : S X R X S X A \\to [0,1]$ is an ordinary deterministic function of 4 arguments:\n",
        "\n",
        "$$\\sum_{s' \\epsilon S} \\sum_{r \\epsilon R} p(s',r|s,a) = 1, \\text{for all } s \\epsilon S, a \\epsilon A(s).$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4fddh_JI5ox5",
        "colab_type": "text"
      },
      "source": [
        "**This means:**\n",
        "- probability of current state and rewards *only depends on* the immediate preceding state and action (Definition of 1st order Markov processes).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9dtOrZVW7fxP",
        "colab_type": "text"
      },
      "source": [
        "### **Example: A recycling robot ðŸ¤–**\n",
        "[from Sutton and Barto: Reinforcement Learning An Introduction 2nd Edition, Chapter 3 example 3.3](http://incompleteideas.net/book/RLbook2020.pdf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BoqjtyPl7zIK",
        "colab_type": "text"
      },
      "source": [
        "A mobile robot has the job of collecting empty soda cans in an office engironment. High-level decisions about how to search for cans are made by a RL agent *based on the current charge level of the battery* (aka the state):\n",
        "\n",
        "- **The battery assumes 2 charge levels - high and low:** $S=\\{\\text{high, low}\\}$\n",
        "- **The agent can choose to search, wait or recharge:** $A=\\{\\text{search, wait, recharge}\\}$\n",
        "- **The agent receives the following rewards for the following situations:**\n",
        "    - searching for a can depletes battery. if:\n",
        "        - robot finds a soda can, *positive reward* $r_{search}$\n",
        "        - battery becomes depleted, *large negative reward* $-3$\n",
        "        - shuts down and wait to be rescued, *positive reward* $r_{wait}$\n",
        "- **The robot has the following probabilities:**\n",
        "    - $\\alpha$: probability of robot having a high charge ending with a low charge after searching\n",
        "    - $\\beta$: probability of robot having low charge end ending up with a depleted charge\n",
        "\n",
        "**Below shows the `transition graph` of this finite MDP process:**\n",
        "![alt text](https://github.com/shengy90/reinforcement-learning-an-introduction/blob/master/misc/ch3fig2.png?raw=true)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xcKm89USi7lD",
        "colab_type": "text"
      },
      "source": [
        "# 3.2 Goals and Rewards"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-w920hPD-y5T",
        "colab_type": "text"
      },
      "source": [
        "**Rewards:** a numerical value $R_{t} \\epsilon \\mathbb{R}$ that the `environment` passes to the `agent` \n",
        "\n",
        "**Goals:** maximise the rewards that the agent will receive *in the long run*\n",
        "\n",
        "**Very often:**\n",
        "- researches have also provided a reward of $-1$ for every timestep that passes to incentivise the agent finding the fastest possible solution. \n",
        "- also we need to give only appropriate rewards, i.e. make sure that when the agent maximises the reward, it also maximises our goals, not the 'sub goals'.\n",
        "- e.g. in a chess game, if we reward the agent for every opponent's piece removed, the robot might be maximising the number of pieces removed and still lose the game (thereby achieving subgoals but not the main goal).\n",
        "- reward signal is our way of communicating *what* we want achieved, not *how*!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mOcz_8rli9dP",
        "colab_type": "text"
      },
      "source": [
        "# 3.3 Returns and Episodes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ybfEguKARiK",
        "colab_type": "text"
      },
      "source": [
        "**`Expected return`** \n",
        "\n",
        "The sum of all future rewards the agent expects to get: $G_{t} \\dot = \\sum_{i=t}^{T} T_{i+1} $\n",
        "\n",
        "**`Episodes`:** \n",
        "\n",
        "A *subsequence* e.g. plays of game, or tips through a maze, or any sort of repeated interaction. Each episode ends in a 'special state' called the `terminal state`, followed by a *reset* to a standard starting state (or to a sample from a standard distribution of starting states). The next episode begins independently of how the previous ended. Tasks with episodes are called `episodic tasks` where:\n",
        "- $S$ : nonterminal states \n",
        "- $S^{+}$: terminal state \n",
        "\n",
        "**`Continuing tasks`:**\n",
        "\n",
        "Noncontinuing tasks where time could go on to infinity. This makes the reward expression above problematic, because what we're trying to maximise is then 'infinite' i.e. impossible! To combat this, we can introduce a notion of `discounting`. \n",
        "\n",
        "**`Discounting`:**\n",
        "$$G_{t} \\dot = \\sum_{k=0}^{\\infty} \\gamma^{k} R_{t+k+1},$$\n",
        "\n",
        "Where $0 \\leq \\gamma \\leq 1$ is called the `discount rate`. Gamma basically weights immediate rewards higher than some future distant reward. So getting a +1 in t=t+10 is worth more than at t=t+20. \n",
        "\n",
        "- $\\gamma$ = 0 : agent is said to be *myopic* in the sense that it only cares about immediate reward\n",
        "- $\\gamma% = 1 : agent takes into account future rewards much more strongly \n",
        "\n",
        "Introducing the `discount rate` solves the infinity problem because as time tends to infinity:\n",
        "$$G_{t} = \\sum_{k=0}^{\\infty} \\gamma^k = \\frac{1}{1-\\gamma}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J3-RHBB7jBZg",
        "colab_type": "text"
      },
      "source": [
        "# 3.4 Unified Notation for Episodic and Continuing Tasks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-R2FsBOMIYLk",
        "colab_type": "text"
      },
      "source": [
        "To generalise notation so that we can include both episodic and continuing tasks, we'll define the reward function as:\n",
        "\n",
        "$$G_t \\dot = \\sum_{k=t+1}^{T} \\gamma^{k-t-1} R_{k},$$\n",
        "\n",
        "Where $T=\\infty$ (continuing tasks) or $\\gamma=1$ (episodic tasks) but not both. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pNgTVjTrjHSB",
        "colab_type": "text"
      },
      "source": [
        "# 3.5 Policies and Value Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "srjTaRBfNgJ8",
        "colab_type": "text"
      },
      "source": [
        "#### **Definitions**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TKcEI4JcI4sI",
        "colab_type": "text"
      },
      "source": [
        "**`Value functions`**\n",
        "\n",
        "Functions of states that estimate *how good* it is for the agent to be in a given state/ perform a given action in a given state. It measures the 'expected return' with respect to `policies`. \n",
        "\n",
        "**`Policy`**\n",
        "\n",
        "A policy is a mapping from states to probabilities of selecting each possible actions. If an agent is following policy $\\pi$ at time $t$, then $\\pi(a|s)$ is the probability that $A_{t}=a$ if $S_{t}=s$.\n",
        "\n",
        "Reinforcement learing methods specify how the agent's policy is changed as a result of its experience. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uk9tLTWDK695",
        "colab_type": "text"
      },
      "source": [
        "The `value function` of a state $s$ under a policy $\\pi$, denoted $v_{\\pi}(s)$, is the `expected return` when starting in $s$ and following $\\pi$ thereafter. For MDPs:\n",
        "\n",
        "$$v_{\\pi}(s) \\space \\dot \\space = \\space \\mathbb{E}_{\\pi}\\big[G_t | S_t=s \\big] = \\mathbb{E}_{\\pi} \\Bigg[\\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1} \\Bigg| S_t=s \\Bigg], \\text{ for all } s \\epsilon S $$\n",
        "\n",
        "We call $v_{\\pi}$ the *state-value function for policy $\\pi$*.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AbG8MTDyMRJK",
        "colab_type": "text"
      },
      "source": [
        "The value of taking action $a$ in state $s$ under a policy $\\pi$ is therefore $q_\\pi(s,a)$:\n",
        "\n",
        "$$q_\\pi(s,a) \\space \\dot = \\space \\mathbb{E}_\\pi \\big[ G_t | S_t=s,A_t=a \\big] = \\mathbb{E}_\\pi \\Bigg[\\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1} \\Bigg| S_t=s, A_t=a \\Bigg]$$.\n",
        "\n",
        "We call $q_\\pi(s,a)$ the *action-value function for policy $\\pi$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6r5b7CSyNkY9",
        "colab_type": "text"
      },
      "source": [
        "#### **Estimating Value Functions**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SIGGr2myNnIn",
        "colab_type": "text"
      },
      "source": [
        "The value functions $v_\\pi$ and $q_\\pi$ can be estimated from experience:\n",
        "- if an agent follows policy $\\pi$ and maintains an average, for each state encountered, of the actual returns that have followed the state, then the average will converge to the state's value, $v_\\pi(s)$. \n",
        "- if separate averages are kept for each action taken in each state, these averages will converge to the action values $q_\\pi(s,a).$\n",
        "- this is also called *Monte Carlo method* as it involves average over many random samples of actual returns. \n",
        "- this might not be practical for systems with many many states! for such systems, we need something more complex."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F0EAHFDTWJWW",
        "colab_type": "text"
      },
      "source": [
        "**One fundamental property of value functions in RL** is that they satisfy recursive relationships. Meaning for any policy $\\pi$ and state $s$, the following consistency condition holds:\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "v_{\\pi}(s) \n",
        "& \\space \\dot = \\space \\mathbb{E}_\\pi [G_{t} | S_{t} = s] \\\\\n",
        "&= \\mathbb{E}_\\pi [R_{t+1} + \\gamma G_{t+1} | S_{t}=s] \\\\ \n",
        "&= \\sum_a \\pi(a|s) \\sum_{s'}\\sum_{r} p(s',r | s,a) \\big[r + \\gamma \\mathbb{E}_\\pi[G_{t+1}|S_{t+1}=s]\\big] \\\\\n",
        "&= \\sum_a \\pi(a|s) \\sum_{s',r} p(s',r|s,a) \\big[r + \\gamma v_\\pi(s') \\big], \\space \\text{for all }s \\epsilon S\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "where $s'$ refers to the next state. This equation, is basically just the sum over all values of the three variables: $a$, $s'$ and $r$. \n",
        "\n",
        "For each triple, we compute its probability $\\pi(a|s)p(s',r|s,a)$, weigh the quantity in brackets by that probability, then sum over all probabilities to get the expected value. \n",
        "\n",
        "This equation is also called the `Bellman equation` for $v_\\pi$. It expresses the relationship between the value of a state and its successor states. \n",
        "\n",
        "![alt text](https://github.com/shengy90/reinforcement-learning-an-introduction/blob/master/misc/bellman.png?raw=true)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3QqkPKPZWBP9",
        "colab_type": "text"
      },
      "source": [
        "**What Bellman's equation is saying is basically:** the value of the current state should be equal to the discounted value of the expevted next state, plus any rewards expected along the way."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a-TDRlL6de5O",
        "colab_type": "text"
      },
      "source": [
        "#### **Gridworld Example**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E5EQHUZfdhj0",
        "colab_type": "text"
      },
      "source": [
        "![gridworld](https://github.com/shengy90/reinforcement-learning-an-introduction/blob/master/misc/Screenshot%202020-05-06%20at%2015.49.15.png?raw=true)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mQOae6bSd5yI",
        "colab_type": "text"
      },
      "source": [
        "Taken from [`Reinforcement Learning : An Introduction` by Sutton and Barto.](http://incompleteideas.net/book/RLbook2020.pdf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E1-z6RaNeGeh",
        "colab_type": "text"
      },
      "source": [
        "**Left figure shows the retangular gridworld representation of a finite MDP. Each cell corresponds to the states of the `environment`.**\n",
        "- There are 4 possible actions at each cell: move 1 grid *north*, *south*, *east* or *west*. \n",
        "- Actions that result in the agent leaving the grid has no effect, and returns -1 reward. \n",
        "- From State A, all 4 actions will move the agent to A' and get 10 points. \n",
        "- From state B, all 4 actions will move the agent to B' and get 5 points.\n",
        "- All other actions get 0 points. \n",
        "\n",
        "**Right figure shows $v_\\pi$, the state-value function for the equiprobable random policy with discount rate of $\\gamma$ = 0.9.**\n",
        "- Notice negative values near the lower edge - these are results of the high probability of hitting the edge under this policy\n",
        "- State A has the highest value under this 'equiprobable policy'. Note that it's value is < 10, because as a result of landing in state A, the agent would end up having a high chance of falling off the grid.\n",
        "- State B on the other hand, is valued more than its immediate reward of 5 as ending up in B' is still a relatively safe spot which *might* allow the user to end up in A or B again and getting the additional points. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yrfwLHpkdyvn",
        "colab_type": "text"
      },
      "source": [
        "**TO:DO** CODE AND SOLVE THIS PROBLEM."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r-fjmqMFjOR-",
        "colab_type": "text"
      },
      "source": [
        "# 3.6 Optimal Policies and Optimal Value Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5aygkTdMgsW-",
        "colab_type": "text"
      },
      "source": [
        "Value functions can be use to rank different policies, i.e. if policy $\\pi$ is equal to or better than $\\pi'$, then its expected return is also greater than or equal to that of $\\pi'$ for all states. In short: $\\pi \\geq \\pi'$ if and only if $v_{\\pi}(s) \\geq v_{\\pi'}(s)$ for all $s\\epsilon S. \n",
        "\n",
        "The *optimal policy*  (denoted as $\\pi_*$) is the policy that's better than or equal to all other policies. Its state-value function, is defined as:\n",
        "$$v_*(s) \\dot = \\max_\\pi v_\\pi(s) \\space \\text{ for all } s \\epsilon S.$$\n",
        "\n",
        "\n",
        "**Note that**  because $v_*$ is the value function for a poilcy, it must also satisfy the self-consistent condition giben by the Bellman equation. And because this is the optimal value function, it could be written in a special form without reference to any specific policy:\n",
        "\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "v_*(s) \n",
        "&= \\max_{a \\epsilon A(s)} q_{\\pi_{*}}(s,a) \\\\\n",
        "&=\\max_{a} \\mathbb{E}_{\\pi_*} \\big[G_t | S_t=s, A_t=a \\big] \\\\\n",
        "&=\\max_{a} \\mathbb{E}_{\\pi_*} \\big[R_{t+1} + \\gamma G_{t+1} | S_t=s, A_t=a \\big] \\\\\n",
        "&=\\max_{a} \\mathbb{E} \\big[ R_{t+1} + \\gamma v_* (S_{t+1}) | S_t=s, A_t=a \\big] \\\\ \n",
        "&=\\max_{a} \\sum_{s',r} p(s',r|s,a)[r + \\gamma v_*(s')]\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "The Bellman optimality equation for $q_*$ is:\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "q_*(s,a) \n",
        "&= \\mathbb{E} \\big[ R_{t+1} + \\gamma \\max_{a'} q_*(S_{t+1}, a') | S_t=s, A_t=a \\big] \\\\\n",
        "&= \\sum_{s',r} p(s',r|s,a) [r+ \\gamma \\max_{a'} q_*(S',\n",
        "\\end{align}\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yfnXrMCvpuE9",
        "colab_type": "text"
      },
      "source": [
        "### **Optimal solution to gridworld**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wGUavUGMp9Tm",
        "colab_type": "text"
      },
      "source": [
        "![alt text](https://github.com/shengy90/reinforcement-learning-an-introduction/blob/master/misc/Screenshot%202020-05-06%20at%2016.42.13.png?raw=true)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CG_RKeNMp___",
        "colab_type": "text"
      },
      "source": [
        "The figures above show the optimal solution to gridworld example. $v_*$ shows the state value of every given state, whilst $\\pi_*$ shows the corresponding optimal policies where multiple arrows mean all those arrow have the equal action-values."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3DlnQaVEqctj",
        "colab_type": "text"
      },
      "source": [
        "### **Optimal solution to recycling robot**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J8kSGdNwqgF6",
        "colab_type": "text"
      },
      "source": [
        "**Recall that for the recycling robot problem:**\n",
        "- 2 states: high (h), low(l)\n",
        "- 3 actions : search (s), wait (w) and recharge(re)\n",
        "\n",
        "Since there are only 2 states, Bellman optimality equation therefore consists only of 2 equations: $v_*(h)$ and $v_*(l)$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VVro6lz_wj4B",
        "colab_type": "text"
      },
      "source": [
        "#### **Equation for $v_*(h)$ (optimal value function for high battery levels**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gKrU6LVF3jmY",
        "colab_type": "text"
      },
      "source": [
        "**Recall that in high battery state, the policy is *do not recharge***.\n",
        "- Action 1: search with s'=(high or low)\n",
        "- Action 2: wait with s'=(high or low)\n",
        "\n",
        "1. **Action 1: Seach with s'=high** :=> $v(h,s) = p(h|h,s)[r(h,s,h) + \\gamma v_*(h)$\n",
        "2. **Action 1: Search with s'=low** :=> $v(l,s) = p(h|h,s)[r(h,s,l) + \\gamma v_*(l)$\n",
        "3. **Action 2: Wait with s'=high** :=> $v(h,w) = p(h|h,w)[r(h,s,h) + \\gamma v_*(h)$\n",
        "4. **Action 2: Wait with s'=low**  :=> $v(l,w) = p(h|h,w)[r(h,s,l) + \\gamma v_*(l)$\n",
        "\n",
        "Therefore:\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "v_*(h) \n",
        "&= \\max\\bigg(v(h,s)+v(l,s), v(h,w)+v(l,w) \\bigg) \\\\\n",
        "&= \\max\\bigg(\n",
        "    \\alpha[r_s + \\gamma v_*(h) + (1-\\alpha)[r_s + \\gamma v_*(l)], \n",
        "    1[r_w+\\gamma v_*(h)] + 0[r_w + \\gamma v_*(l)] \\bigg) \\\\\n",
        "&= \\max\\bigg(r_s + \\gamma[\\alpha v_*(h) + (1-\\alpha) v_*(l)],\n",
        "    r_w + \\gamma v_*(h)\\bigg)\n",
        "\\end{align}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JrusWawU3ef3",
        "colab_type": "text"
      },
      "source": [
        "#### **Equation for $v_*(l)$ (optimal value function for low battery levels**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ayUY_s8G3pIR",
        "colab_type": "text"
      },
      "source": [
        "**Recall that in low battery state, the policy is *do not search***.\n",
        "Applying the same approach as above, we get:\n",
        "\n",
        "$$\n",
        "v_*(l) = max\\bigg(\\\\\n",
        "\\beta r_s -3(1-\\beta) + \\gamma[1-\\beta)v_*(h) + \\beta v_*(l)], \\\\\n",
        "r_w \\gamma v_*(l), \\\\\n",
        "\\gamma v_*(h) \\\\\n",
        "\\bigg)\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CkmkfXrujSOw",
        "colab_type": "text"
      },
      "source": [
        "# 3.7 Optimality and Approximation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-IY4UVog40S9",
        "colab_type": "text"
      },
      "source": [
        "- Although it's possible to solve for the value function for the optimal policy theoretically, it's often not possible (without high computational cost) in the real world\n",
        "- Therefore in the next few chapters, we'll look at different numerical approximation that allows us to approximate the optimal policies."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RCFFxh_Q5NhU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}